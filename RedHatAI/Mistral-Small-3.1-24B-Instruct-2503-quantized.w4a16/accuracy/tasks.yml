tasks:
  - name: arc_challenge
    metrics:
      - name: acc_norm,none
        value: 0.7218

  - name: gsm8k
    metrics:
      - name: strict_match,none
        value: 0.6634

  - name: hellaswag
    metrics:
      - name: acc_norm,none
        value: 0.8325

  - name: mmlu
    metrics:
      - name: acc,none
        value: 0.7974

  - name: truthfulqa_mc2
    metrics:
      - name: acc,none
        value: 0.6956

  - name: winogrande
    metrics:
      - name: acc,none
        value: 0.8343

  # following are placeholders for mid-level "leaderboard_*" tasks
  # (OpenLLM v2) waiting for info on how to calculate the metric
  # values from the individual sub tasks.

  # - name: leaderboard_gpqa_main
  #   metrics:
  #     - name: acc-norm,none
  #       value: 0.471

  # - name: leaderboard_gpqa_diamond
  #   metrics:
  #     - name: acc-norm,none
  #       value: 0.4495

  # - name: leaderboard_mmlu_pro
  #   metrics:
  #     - name: acc,none
  #       value: 0.6656

  # - name: humaneval
  #   metrics:
  #     - name: exact_match,none
  #       value: 0.846
