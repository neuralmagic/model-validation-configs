# server configs for https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
model: "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
trust-remote-code: true
enable-chunked-prefill: true
tensor-parallel-size: 1
max-model-len: 4096
