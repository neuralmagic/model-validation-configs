# Note: The container-only chat template path is intentionally omitted here to
# keep non-container CI runs working. If running inside the vLLM container image,
# supply the chat template path via an override config.
enable-chunked-prefill: true
max-model-len: 4096
tensor-parallel-size: 1
trust-remote-code: true
enable-auto-tool-choice: true
tool-call-parser: llama3_json
uvicorn-log-level: debug
